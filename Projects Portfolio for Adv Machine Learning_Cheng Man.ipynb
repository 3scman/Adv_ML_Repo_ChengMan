{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projects Portfolio - Advanced Machine Learning\n",
    "Cheng Man  \n",
    "2020/05/10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This file is a Portfolio for my 2020 Spring Advanced Machine Learning projects. There are three projects described in this document, and their Github links follow behind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The url of my Github Repo is: https://github.com/3scman/Adv_ML_Repo_ChengMan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project #1: U.N. World Happiness Data Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What makes the citizens of one country more happy than the citizens of other countries? Do variables measuing perceptions of corruption, GDP, maintaining a healthy lifestyle, or social support associate with a country's happiness ranking?\n",
    "\n",
    "This report uses the United Nation's World Happiness Rankings country level data to experiment with models that predict happiness rankings well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The url of this project is: https://github.com/3scman/Adv_ML_Repo_ChengMan/tree/master/HW01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data used in this project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data**: 2019 World Happiness Survey Rankings + ISO-3166-Countries-with-Regional-Codes\n",
    "\n",
    "**Features**\n",
    "\n",
    "*   Region\n",
    "*   GDP per capita\n",
    "*   Social support\n",
    "*   Healthy life expectancy\n",
    "*   Freedom to make life choices\n",
    "*   Generosity\n",
    "*   Perceptions of corruption\n",
    "\n",
    "**Target**\n",
    "*   Happiness_level (Very High = Top 20% and Very Low = Bottom 20%)\n",
    "\n",
    "\n",
    "**Source**: https://worldhappiness.report/;  \n",
    "        https://github.com/lukes/ISO-3166-Countries-with-Regional-Codes/blob/master/all/all.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods used in this project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explore the data**: Using `ExtraTreesClassifier()` to examine feature importance\n",
    "\n",
    "**Preprocess**: Different preprocessing methods for numeric and categorical variables.\n",
    "\n",
    "*   For numeric variables, the project uses `SimpleImputer()` with `strategy='median'`, and `StandardScaler()` to scale the data.\n",
    "*   For categorical variables, the project uses `SimpleImputer()` with `strategy='most_frequent'`, and `OneHotEncoder()`.\n",
    "\n",
    "**Model #1**: Neural Network with Keras\n",
    "\n",
    "* In this Keras Deep Learning Model, there are multiple parameters:  \n",
    "\n",
    "* 1.  Optimizer  \n",
    "    >I instantiated an optimizer before passing it to the `model.compile()`, and I choose `Adadelta()` as the optimizer. From the `Keras` Document, \"Adadelta is a more robust extension of `Adagrad` that adapts learning rates based on a moving window of gradient updates, instead of accumulating all past gradients. This way, Adadelta continues learning even when many updates have been done.\"  \n",
    "    \n",
    "    >People can find the document about optimizer in `Keras` from [this website](https://keras.io/optimizers/).  \n",
    "    \n",
    "    >Choosing Adadelta() gave me the best test scores compared to other optimizer like `SGD`, `Adam`, `Adamax`, etc. \n",
    "\n",
    "* 2.  Learning Rate `lr`   \n",
    "    >The learning rate is a tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward a minimum of a loss function. When I instantiated the optimizer, I set up the learning rate in this model. The learning rate, which is `lr` in the code, is **0.1** this time. After setting that, I got a better result. \n",
    "\n",
    "* 3.  `batch_size`, `epochs` and `validation_split`  \n",
    "    >The `batch_size` defines the number of samples that will be propagated through the network.   \n",
    "\n",
    "    >The `epochs` means the number of epochs to train the model. An epoch is an iteration over the entire x and y data provided.  \n",
    "\n",
    "    >The `validation_split` is the fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch.   \n",
    "\n",
    "* I tried to run a `GridSearchCV` to find better `batch_size` and `epochs` for this model. For `validation_split`, I tried different values between **0 to 0.2**. Since the training data set is relatively small in this case, I decided to set it to **0**, and it gave me a better result.\n",
    "\n",
    "**Model #2**: KNN\n",
    "\n",
    "* The parameters in this KNN model include `n_neighbors` and `weights`. I used `GridSearchCV` to find the best parameter combination.  \n",
    "\n",
    "> `n_neighbors`: Number of neighbors to use in the model. In this case, after grid search, the best value is **4**.  \n",
    "\n",
    "> `weights`: There are two kinds of weights in a sklearn KNN model.  \n",
    "* `uniform` : uniform weights. All points in each neighborhood are weighted equally.  \n",
    "* `distance` : weight points by the inverse of their distance. in this case, closer neighbors of a query point will have a greater influence than neighbors which are further away. \n",
    "* After grid search, the better method is `distance`.  \n",
    "\n",
    "**Model #3**: Random Forest\n",
    "\n",
    "* The parameters in this Random Forest model include `max_depth` and `n_estimators`. I used `GridSearchCV` to find the best parameter combination.\n",
    "\n",
    "> `max_depth`: The maximum depth of the tree. In this model, it is **11**.\n",
    "\n",
    "> `n_estimators`:  The number of trees in the forest. In this model, the number of trees is **100**.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "> In this report, the final model I would like to choose is Keras Deep Learning Model. This model predicts the best among the three models discussed above. Shown by the evaluation metrics, Keras has the better result.  \n",
    "\n",
    "> However, I do not believe that deep learning is simply better than traditional machine learning methods, since the original data is the governor which decides the quality of the prediction outcomes. In this World Happiness Analysis, the original dataset is quite small, having only 117 observations in the training dataset. \n",
    "\n",
    "> There is a saying that deep learning is better dealing with huge scale of data, however, it did better in this small case. This is partly due to the randomness. if I change those `random_state`, I probably get different result. From my view, I think Random Forest is also a good model for this dataset prediction analysis, since it have a 0.64 accuracy. It's pretty good as well. But following those metrics only, my final decision is choosing the deep learning model, which I already uploaded in Part 1.4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project #2: Brain Tumor Picture Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project makes use of brain tumor diagnostic MRI image data to predict whether a picture has brain tumor or not.  \n",
    "\n",
    "When building a model like this, which can judge a brain X-ray figure whether having or tumor or not, can be helpful in two kinds of aspects.\n",
    "\n",
    ">1. First, for doctors, this model helps them to archive these files easier. As this model, though it is simple that can only distinguish having tumor or not. However, keep upgrade the model, can even help doctors seperate different kinds of tumors. This process will be helpful for their researcheses or diagnosis.\n",
    "\n",
    ">2. Second, for patients, since they have less professional knowledge towards their X-ray photos, using this model can help them find out whether their X-ray photos tell them they have a brain tumor or not. This can be a self-diagnosis process. Further, if upgrade the model, people can even find out the most likely situations. Base on the other patient's treatment and experiences, they can get more information and be fully prepared."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The url of this project is: https://github.com/3scman/Adv_ML_Repo_ChengMan/tree/master/HW02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data used in this project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data**: 253 Brain MRI images. 155 with brain tumor and 98 without.\n",
    "\n",
    "**Format**: JPEG with different size.\n",
    "\n",
    "**Target**: Predict whether an image has brain tumor or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods used in this project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocess**:\n",
    "*  Reshape every image to 224 * 224 * 3\n",
    "*  Min Max Transformation\n",
    "\n",
    "**Model #1**: a basic convnet model\n",
    "\n",
    "* Three dense layers before flatten.\n",
    "* Hidden node is set to 64 for all three layers.\n",
    "* Set `ReduceLROnPlateau()` to divide `lr` by 2 when `val_accuracy` fails to improve after 2 epochs.\n",
    "\n",
    "**Model #2**: a more complex convnet model\n",
    "\n",
    "* Two Conv2D layers followed by a Maxpooling layer.\n",
    "* `kernal_size` set to 3 for the first Conv2D and 1 for the second Conv2D; `filter` is set to 32; `padding` is `same`\n",
    "* Also set the same `ReduceLROnPlateau()`.\n",
    "\n",
    "**Model #3**: a transfer learning convnet model\n",
    "\n",
    "* Use `VGG16` as the base model\n",
    "* Add a GlobalAveragePooling2D() followed.\n",
    "* `optimizer` is `Adam` with learning rate set to 0.01\n",
    "* Set `ReduceLROnPlateau()` to divide `lr` by 2 when `val_accuracy` fails to improve after 4 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "The best model I created is the Model3. I used VGG16 as the base model, added a pooling layer and a sigmoid function to get the binary result. Also, I tried to add a learning rate reduce function, using `ReduceLROnPlateau()`, which allow me to reduce the learning rate in the `.fit` process.\n",
    "\n",
    "Here are some relevant hyper-parameter values:  \n",
    "\n",
    "* 1.  Optimizer  \n",
    "The optimizer I chose is `Adam()`. From the documentation, \"*Adam optimization is a stochastic gradient descent method that is based on adaptive estimation of first-order and second-order moments*\". I set the basic learning rate in the optimizer function as well.\n",
    "\n",
    "* 2.  Learning rate\n",
    "The basic learning rate I set is 0.01. As I mentioned, in `red_lr` I set a learning rate reduce principle. If after 4 epochs the `val_accuracy` fails to improve, the learning rate will be divided by 2. The minimal learning rate is set to 0.001 in order to avoid it getting too low.\n",
    "\n",
    "* 3.  Epochs\n",
    "I set the epochs to 25. The `epochs` means the number of epochs to train the model. An epoch is an iteration over the entire x and y data provided. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project #3: BBC News Category Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project wants to predict the category of a BBC news by the raw text of that news."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The url of this project is: https://github.com/3scman/Adv_ML_Repo_ChengMan/tree/master/HW03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data used in this project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description**\n",
    "\n",
    "* For this BBC dataset, every single row represents a news article from BBC.  \n",
    "\n",
    "\n",
    "* The dataset has two columns: \n",
    "\n",
    " 1. category(which category the article is belong) \n",
    " 2. text(the raw text in the article). \n",
    "\n",
    "**Target**\n",
    "\n",
    "* There are 2,225 articles in the dataset from 5 categories: tech, business, sport, entertainment and politics.  \n",
    "\n",
    "Source: https://storage.googleapis.com/dataset-uploader/bbc/bbc-text.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods used in this project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocess**\n",
    "\n",
    "* Make each document in the data is represented as a sequence of equal length\n",
    "* Use `Tokenizer` in `Keras` package to tokenize the documents.\n",
    "\n",
    "**Models**\n",
    "\n",
    "* A model with an embedding layer and dense layers (but w/ no layers meant for sequential data)  \n",
    "\n",
    "* Using an Embedding layer with Conv1d Layers  \n",
    "\n",
    "* Using an Embedding layer with one LSTM layer  \n",
    "\n",
    "* Using an Embedding layer with stacked LSTM layers  \n",
    "\n",
    "* Using an Embedding layer with bidirectional LSTM layers  \n",
    "\n",
    "* Using dropout in the bidirectional layer Model  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "1. In this project, the best model is Model #2, which uses an Embedding layer with Conv1d Layers.  \n",
    "\n",
    "2. For further improving my model, I think there are several ways, including:  \n",
    "1) **Use Glove embedding matrix weights.**Words not found in Glove embedding index will be all-zeros.   \n",
    "2) **Add more layers to increase the depth of the model**, making the model more accurately earning the description as a deep learning technique.  \n",
    "3) **Combining different kinds of layers in one single model**, in order to make the model more comprehensive.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
